### **LEVEL 3: Open-Ended Challenge** ⭐⭐⭐
*Tests critical thinking.*

**Goal:** Show me HOW you think about problems

**Pick ONE of these challenges:**

---

#### **OPTION A: Break the Model**

**Task:** Find ways to make the model fail or produce bad outputs

Try to make it:
- Repeat itself endlessly
- Generate nonsense/gibberish
- Contradict itself
- Get stuck in loops
- Produce offensive content (document but don't share)
- Any other failure mode

**Submit:**
- At least 5 different failure modes
- Example outputs for each
- Explanation of WHY each failure happens
- Ideas for how to fix it

---

#### **OPTION B: Measure Quality**

**Task:** Create a scoring system for generated text quality

Define "quality" (coherence? creativity? grammar? relevance?) then build code to measure it.

Requirements:
- Score text from 0-100
- Test on at least 10 generated texts
- Show examples of high-scoring vs low-scoring
- Explain: does your metric actually correlate with good text?

**Submit:**
- Your quality metric code
- Scores for 10 texts with explanations
- Reflection: what makes text "high quality"?

---

#### **OPTION C: Compare Models**

**Task:** Test 3+ different models and compare them

Models to try:
- `distilgpt2` (current)
- `gpt2` (larger)
- `gpt2-medium` (even larger)
- Or feel free to find others on Hugging Face

Compare:
- Generation speed
- Output quality (subjective)
- Model size
- When to use each

**Submit:**
- Comparison table (speed, quality, size)
- Same prompts tested on all models
- Recommendation: which model for which use case?

---

#### **OPTION D: Make Your Own Challenge**

If you have a different idea related to text generation, propose it and do it.

**Must include:**
- Clear goal
- Code implementation
- Results/findings
- Analysis

---

**If you complete Level 3 → You're ready for Tier 2-3 tasks**

